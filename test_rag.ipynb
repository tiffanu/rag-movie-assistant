{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40f255e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "import evaluation\n",
    "\n",
    "from langsmith.evaluation import evaluate\n",
    "from langchain import hub\n",
    "from typing import Dict, Any\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain import hub\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d318ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "MISTRAL_API_KEY = #your_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1225d215",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_rag import MovieRAGWithDeepSearch\n",
    "\n",
    "rag = MovieRAGWithDeepSearch(csv_path=\"data/TMDB_all_movies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8a7dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_single_name = \"RAG Movie Expert Test Dataset\"\n",
    "\n",
    "PATH_TO_DATASET=\"test_queries_mistral.json\"\n",
    "\n",
    "evaluation.create_langsmith_dataset(json_file=PATH_TO_DATASET, dataset_name=dataset_single_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff19970",
   "metadata": {},
   "source": [
    "Answer vs Reference Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69990a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
    "\n",
    "llm_grader = ChatMistralAI(\n",
    "    model=\"mistral-large-latest\",\n",
    "    temperature=0,\n",
    "    max_retries=2,\n",
    "    api_key=MISTRAL_API_KEY,\n",
    ")\n",
    "\n",
    "def rag_predictor(example: Dict[str, Any]) -> Dict[str, str]:\n",
    "\n",
    "    question = example[\"query\"]\n",
    "    \n",
    "    response = rag.rag_function({\"query\": question})\n",
    "    \n",
    "    return {\n",
    "        \"answer\": response[\"answer\"],\n",
    "    }\n",
    "\n",
    "manual_grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"\"\"You are a grader assessing the accuracy of a student's answer given a question and a reference answer.\n",
    "        \n",
    "        You will be given:\n",
    "        1. The Question\n",
    "        2. The Reference Answer (Ground Truth)\n",
    "        3. The Student's Answer (Prediction)\n",
    "\n",
    "        Compare the Student's Answer to the Reference Answer. \n",
    "        If the student's answer conveys the same meaning as the reference answer, score it as 1. \n",
    "        If it is different, but the films are similar give something between 0 and 1 depending how similar are films\n",
    "        If it is incorrect or different, score it as 0.\n",
    "\n",
    "        Return a JSON object with a single key 'Score' and the integer value (0 or 1).\n",
    "        \"\"\"),\n",
    "        (\"human\", \"\"\"Question: {question}\n",
    "        Reference Answer: {correct_answer}\n",
    "        Student Answer: {student_answer}\n",
    "        \"\"\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "def answer_evaluator(run, example) -> dict:\n",
    "    input_question = example.inputs[\"query\"]\n",
    "    reference = example.outputs[\"reference_answer\"]\n",
    "    prediction = run.outputs[\"answer\"]\n",
    "\n",
    "    llm_grader = rag.llm\n",
    "    parser = JsonOutputParser()\n",
    "\n",
    "    answer_grader = manual_grade_prompt | llm_grader | parser\n",
    "\n",
    "    score_result = answer_grader.invoke({\n",
    "        \"question\": input_question,\n",
    "        \"correct_answer\": reference,\n",
    "        \"student_answer\": prediction\n",
    "    })\n",
    "\n",
    "    score = score_result.get(\"Score\", 0)\n",
    "\n",
    "    return {\"key\": \"answer_v_reference_score\", \"score\": score}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffcb1b2",
   "metadata": {},
   "source": [
    "Сравнение с результатами LLM без RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c126b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_evaluator(run, example) -> dict:\n",
    "    input_question = example.inputs[\"query\"]\n",
    "    reference = example.outputs[\"reference_answer\"]\n",
    "    print(run.outputs.keys())\n",
    "    prediction = run.outputs[\"answer\"]\n",
    "\n",
    "    llm_grader = llm_grader\n",
    "    \n",
    "    parser = JsonOutputParser()\n",
    "\n",
    "    answer_grader = manual_grade_prompt | llm_grader | parser\n",
    "\n",
    "    score_result = answer_grader.invoke({\n",
    "        \"question\": input_question,\n",
    "        \"correct_answer\": reference,\n",
    "        \"student_answer\": prediction\n",
    "    })\n",
    "\n",
    "    score = score_result.get(\"Score\", 0)\n",
    "\n",
    "    return {\"key\": \"answer_v_reference_score\", \"score\": score}\n",
    "\n",
    "\n",
    "\n",
    "student_llm = ChatMistralAI(model=\"open-mistral-7b\", temperature=0, api_key=MISTRAL_API_KEY)\n",
    "\n",
    "student_prompt = ChatPromptTemplate.from_template(\"Answer the question: {question}\")\n",
    "student_chain = student_prompt | student_llm | StrOutputParser()\n",
    "\n",
    "\n",
    "def target_function(inputs: dict) -> dict:\n",
    "    response = student_chain.invoke({\"question\": inputs[\"query\"]})\n",
    "    return {\"answer\": response}\n",
    "\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    target_function,\n",
    "    data=dataset_single_name,\n",
    "    evaluators=[answer_evaluator],\n",
    "    experiment_prefix=\"mistral-no-rag-evaluation\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
